import os
import shutil
import json
from glob import glob
from PIL import Image
import face_recognition
import numpy as np
import piexif
from datetime import datetime
from geopy.geocoders import Nominatim
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import spacy
from spacy.cli import download
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
import uuid
import uvicorn


PHOTO_DIR = "photos/"
SORTED_DIR = "sorted_photos/"
ENCODINGS_FILE = "face_encodings.json"
# å…¨å±€å˜é‡
processor = None
model = None
nlp = None




# os.makedirs(SORTED_DIR, exist_ok=True)

# ==========================
# ğŸŒŸ åŠ è½½å·²æœ‰çš„äººè„¸æ•°æ®ï¼ˆJSON æ–‡ä»¶ï¼‰
# ==========================
def load_encodings():
    global next_label_id
    if not os.path.exists(ENCODINGS_FILE):
        print("ğŸ“‚ æ²¡æœ‰æ£€æµ‹åˆ°ç°æœ‰çš„äººè„¸æ•°æ®åº“ï¼Œåˆå§‹åŒ–ä¸ºç©ºã€‚")
        return {}, [], []

    with open(ENCODINGS_FILE, 'r') as f:
        data = json.load(f)

    known_face_dict = {}  # {label: encoding (np array)}
    known_face_encodings = []
    known_face_labels = []

    for label, encoding_list in data.items():
        label_int = int(label)
        encoding_array = np.array(encoding_list)
        known_face_dict[label_int] = encoding_array
        known_face_encodings.append(encoding_array)
        known_face_labels.append(label_int)

    next_label_id = max(known_face_dict.keys(), default=0) + 1
    print(f"âœ… æˆåŠŸåŠ è½½ {len(known_face_dict)} ä¸ªå·²çŸ¥äººç‰©æ•°æ®ã€‚")
    return known_face_dict, known_face_encodings, known_face_labels



# ==========================
# ğŸŒŸ ä¿å­˜äººè„¸ç¼–ç åˆ° JSON
# ==========================
def save_encodings(known_face_dict):
    data_to_save = {}
    for label, encoding_array in known_face_dict.items():
        data_to_save[label] = encoding_array.tolist()  # è½¬æˆ JSON å¯ä¿å­˜çš„ list

    with open(ENCODINGS_FILE, 'w') as f:
        json.dump(data_to_save, f)

    print(f"ğŸ’¾ å·²ä¿å­˜ {len(data_to_save)} ä¸ªäººç‰©çš„ç¼–ç æ•°æ®åˆ° {ENCODINGS_FILE}")


# åˆå§‹åŒ–å‡½æ•°
def init_blip_and_spacy():
    global processor, model, nlp  # å¼•ç”¨å…¨å±€å˜é‡
    try:
        print("æ­£åœ¨åŠ è½½ BLIP æ¨¡å‹å’Œå¤„ç†å™¨...")
        processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        print("BLIP æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½å®Œæˆï¼")
    except Exception as e:
        print(f"åŠ è½½ BLIP æ¨¡å‹æ—¶å‡ºé”™: {e}")
        processor, model = None, None

    try:
        print("æ­£åœ¨æ£€æµ‹å’ŒåŠ è½½ SpaCy è‹±æ–‡æ¨¡å‹...")
        download("en_core_web_sm")  # åªä¸‹è½½ä¸€æ¬¡
        nlp = spacy.load("en_core_web_sm")
        print("SpaCy è‹±æ–‡æ¨¡å‹åŠ è½½å®Œæˆï¼")
    except Exception as e:
        print(f"åŠ è½½ SpaCy æ¨¡å‹æ—¶å‡ºé”™: {e}")
        nlp = None


# ==========================
# ğŸŒŸ å¤„ç†å•å¼ å›¾ç‰‡
# ==========================
def process_new_photo(photo_path):
    global next_label_id
    try:
        # 1. æ‰“å¼€å¹¶è½¬æ¢å›¾ç‰‡
        with Image.open(photo_path) as img:
            img = img.convert("RGB")
            image_array = np.array(img)

        # 2. äººè„¸è¯†åˆ«
        encodings = face_recognition.face_encodings(image_array)

        if not encodings:
            print(f"âš ï¸ å›¾ç‰‡ {photo_path} ä¸åŒ…å«äººè„¸ï¼Œè·³è¿‡å¤„ç†ã€‚")
            return

        print(f"âœ… è¯†åˆ«åˆ° {len(encodings)} å¼ äººè„¸ï¼Œå¼€å§‹åˆ†ç±»...")

        for encoding in encodings:
            if len(known_face_encodings) == 0:
                # æ²¡æœ‰äººè„¸æ•°æ®ï¼Œç›´æ¥æ–°å»ºäººç‰©
                person_label = next_label_id
                next_label_id += 1

                known_face_dict[person_label] = encoding
                known_face_encodings.append(encoding)
                known_face_labels.append(person_label)
                print(f"ğŸ†• æœªæ£€æµ‹åˆ°å·²æœ‰äººç‰©ï¼Œæ–°å»ºäººç‰© {person_label}")

            else:
                # æ¯”è¾ƒä¸å·²çŸ¥äººè„¸çš„ç›¸ä¼¼åº¦
                distances = face_recognition.face_distance(known_face_encodings, encoding)
                min_distance = np.min(distances)
                best_match_index = np.argmin(distances)

                if min_distance < 0.4:  # é˜ˆå€¼å¯ä»¥è°ƒæ•´
                    person_label = known_face_labels[best_match_index]
                    # print(f"ğŸ‘Œ åŒ¹é…åˆ°äººç‰© {person_label}ï¼Œè·ç¦»ä¸º {min_distance:.2f}")
                else:
                    # æ–°äººç‰©
                    person_label = next_label_id
                    next_label_id += 1

                    known_face_dict[person_label] = encoding
                    known_face_encodings.append(encoding)
                    known_face_labels.append(person_label)
                    # print(f"ğŸ†• æ–°å»ºäººç‰© {person_label}ï¼Œè·ç¦»ä¸º {min_distance:.2f}")

            # ä¿å­˜ç…§ç‰‡åˆ°åˆ†ç±»ç›®å½•
            person_folder = os.path.join(SORTED_DIR, f"äººç‰©{person_label}")
            os.makedirs(person_folder, exist_ok=True)
            shutil.copy(photo_path, person_folder)

        # å¤„ç†å®Œå›¾ç‰‡ï¼Œä¿å­˜æœ€æ–°çš„ encodings
        save_encodings(known_face_dict)
        return person_label

    except Exception as e:
        print(f"âŒ å¤„ç† {photo_path} æ—¶å‡ºé”™: {e}")


def get_decimal_from_dms(dms, ref):
    """Convert GPS coordinates in DMS to decimal format."""
    degrees = dms[0][0] / dms[0][1]
    minutes = dms[1][0] / dms[1][1]
    seconds = dms[2][0] / dms[2][1]

    decimal = degrees + (minutes / 60.0) + (seconds / 3600.0)

    if ref in ['S', 'W']:
        decimal = -decimal
    return decimal


def reverse_geocode(lat, lon):
    """Use geopy to reverse geocode latitude and longitude to address."""
    geolocator = Nominatim(user_agent="photo_metadata_app")
    try:
        location = geolocator.reverse((lat, lon), exactly_one=True, language='en')
        if location:
            return location.address
        else:
            return "Address not found"
    except Exception as e:
        return f"Error retrieving address: {e}"


def extract_exif_data(image_path):
    # æ‰“å¼€å›¾ç‰‡æ–‡ä»¶
    img = Image.open(image_path)

    # è·å– EXIF ä¿¡æ¯
    exif_data = img._getexif()

    # æå–æ‹æ‘„æ—¶é—´
    timestamp = None
    if exif_data and 36867 in exif_data:
        timestamp = exif_data[36867]  # DateTimeOriginal
        timestamp = datetime.strptime(timestamp, '%Y:%m:%d %H:%M:%S')

    # ä½¿ç”¨ piexif åŠ è½½æ›´è¯¦ç»†çš„ EXIF æ•°æ®
    exif_dict = piexif.load(img.info['exif']) if 'exif' in img.info else None

    # æå–ç›¸æœº/è®¾å¤‡ä¿¡æ¯
    camera_model = None
    if exif_dict:
        model = exif_dict['0th'].get(piexif.ImageIFD.Model, None)
        make = exif_dict['0th'].get(piexif.ImageIFD.Make, None)

        camera_model = ""
        if make:
            camera_model += make.decode('utf-8') + " "
        if model:
            camera_model += model.decode('utf-8')

    # æå– GPS ä¿¡æ¯
    gps_info = exif_dict.get('GPS', None) if exif_dict else None
    latitude = longitude = None
    address = None
    if gps_info:
        gps_latitude = gps_info.get(piexif.GPSIFD.GPSLatitude)
        gps_latitude_ref = gps_info.get(piexif.GPSIFD.GPSLatitudeRef).decode('utf-8')
        gps_longitude = gps_info.get(piexif.GPSIFD.GPSLongitude)
        gps_longitude_ref = gps_info.get(piexif.GPSIFD.GPSLongitudeRef).decode('utf-8')

        if gps_latitude and gps_latitude_ref and gps_longitude and gps_longitude_ref:
            latitude = get_decimal_from_dms(gps_latitude, gps_latitude_ref)
            longitude = get_decimal_from_dms(gps_longitude, gps_longitude_ref)

            # åå‘åœ°ç†ç¼–ç è·å–åœ°å€
            address = reverse_geocode(latitude, longitude)

    # è¿”å›æå–çš„ä¿¡æ¯
    return {
        'Timestamp': timestamp,
        'Latitude': latitude,
        'Longitude': longitude,
        'Address': address,
        'Camera/Device': camera_model
    }


# ç”Ÿæˆå›¾ç‰‡æè¿°
def generate_caption(image_path):
    try:
        raw_image = Image.open(image_path).convert('RGB')
        inputs = processor(raw_image, return_tensors="pt")
        out = model.generate(**inputs)
        caption = processor.decode(out[0], skip_special_tokens=True)
        return caption
    except Exception as e:
        print(f"Caption generation error for {image_path}: {e}")
        return "No description available."


def extract_noun_tags(text):
    """
    æå–è‹±æ–‡æ–‡æœ¬ä¸­çš„å…³é”®è¯ï¼ˆåè¯å’Œä¸“æœ‰åè¯ï¼‰

    å‚æ•°:
        text (str): è¾“å…¥çš„è‹±æ–‡æ–‡æœ¬

    è¿”å›:
        tags (list): å»é‡ä¸”å°å†™å¤„ç†åçš„å…³é”®è¯åˆ—è¡¨
    """
    if not text:
        return []

    # åˆ†è¯ & è¯æ€§æ ‡æ³¨
    doc = nlp(text)

    # åªä¿ç•™åè¯ï¼ˆNOUNï¼‰å’Œä¸“æœ‰åè¯ï¼ˆPROPNï¼‰
    tags = [token.text for token in doc if token.pos_ in ['NOUN', 'PROPN']]

    # å»é‡ & å°å†™ï¼ˆå¯æ ¹æ®éœ€æ±‚å–æ¶ˆå°å†™å¤„ç†ï¼‰
    tags = list(set(tag.lower() for tag in tags))

    print(f"Image event tags: {tags}")

    return tags


# å¤„ç†å›¾ç‰‡
def process_images(img_path):
    # æå–å…ƒä¿¡æ¯
    metadata = extract_exif_data(img_path)

    # ç”Ÿæˆæè¿°
    caption = generate_caption(img_path)

    # Aoto-tagging
    tags = extract_noun_tags(caption)

    # äººè„¸è¯†åˆ«
    person_label = process_new_photo(img_path)
    if person_label is not None:
        person_label=[str(person_label)]
    # âœ… å¹³é“º JSON æ ¼å¼
    photo_info = {
        'Timestamp': metadata.get('Timestamp').strftime('%Y-%m-%d %H:%M:%S') if isinstance(metadata.get('Timestamp'),
                                                                                           datetime) else metadata.get(
            'Timestamp'),
        'Latitude': metadata.get('Latitude'),
        'Longitude': metadata.get('Longitude'),
        'Address': metadata.get('Address'),
        'Camera/Device': metadata.get('Camera/Device'),
        'Caption': caption,
        'AutoTags': tags,
        'PersonLabel': person_label
    }

    print(f"Processed {img_path}: {caption} @ {photo_info.get('Address')} on {photo_info.get('Timestamp')}")

    return photo_info

if __name__ == '__main__':
    # ==========================
    # ğŸŒŸ åˆå§‹åŒ–å·²çŸ¥äººè„¸æ•°æ®
    # ==========================
    known_face_dict, known_face_encodings, known_face_labels = load_encodings()

    # è°ƒç”¨åˆå§‹åŒ–å‡½æ•°
    # è°ƒç”¨åˆå§‹åŒ–ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    init_blip_and_spacy()
    app = FastAPI()

    @app.post("/process_image")
    async def process_image(file: UploadFile = File(...)):
        # try:
            # åˆ›å»ºä¸´æ—¶è·¯å¾„ï¼Œé˜²æ­¢æ–‡ä»¶åå†²çª
            temp_filename = f"temp_{uuid.uuid4().hex}_{file.filename}"

            # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶è·¯å¾„
            with open(temp_filename, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)

            # è°ƒç”¨ä½ çš„å›¾ç‰‡å¤„ç†å‡½æ•°ï¼ˆä¼ è·¯å¾„ï¼‰
            result = process_images(temp_filename)

            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å †ç§¯
            os.remove(temp_filename)

            # è¿”å›ç»“æœ
            return JSONResponse(content=result)

        # except Exception as e:
            # return JSONResponse(status_code=500, content={"error": str(e)})


    uvicorn.run(app, host="0.0.0.0", port=8123)