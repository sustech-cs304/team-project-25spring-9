{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T14:23:24.952137Z",
     "start_time": "2025-05-21T14:22:55.758952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import piexif\n",
    "from datetime import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "PHOTO_DIR = \"photos/\"\n",
    "SORTED_DIR = \"sorted_photos/\"\n",
    "ENCODINGS_FILE = \"face_encodings.json\"\n",
    "# å…¨å±€å˜é‡\n",
    "processor = None\n",
    "model = None\n",
    "nlp = None\n",
    "\n",
    "# os.makedirs(SORTED_DIR, exist_ok=True)\n",
    "\n",
    "# ==========================\n",
    "# ğŸŒŸ åŠ è½½å·²æœ‰çš„äººè„¸æ•°æ®ï¼ˆJSON æ–‡ä»¶ï¼‰\n",
    "# ==========================\n",
    "def load_encodings():\n",
    "    global next_label_id\n",
    "    if not os.path.exists(ENCODINGS_FILE):\n",
    "        print(\"ğŸ“‚ æ²¡æœ‰æ£€æµ‹åˆ°ç°æœ‰çš„äººè„¸æ•°æ®åº“ï¼Œåˆå§‹åŒ–ä¸ºç©ºã€‚\")\n",
    "        return {}, [], []\n",
    "\n",
    "    with open(ENCODINGS_FILE, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    known_face_dict = {}  # {label: encoding (np array)}\n",
    "    known_face_encodings = []\n",
    "    known_face_labels = []\n",
    "\n",
    "    for label, encoding_list in data.items():\n",
    "        label_int = int(label)\n",
    "        encoding_array = np.array(encoding_list)\n",
    "        known_face_dict[label_int] = encoding_array\n",
    "        known_face_encodings.append(encoding_array)\n",
    "        known_face_labels.append(label_int)\n",
    "    \n",
    "    next_label_id = max(known_face_dict.keys(), default=0) + 1\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(known_face_dict)} ä¸ªå·²çŸ¥äººç‰©æ•°æ®ã€‚\")\n",
    "    return known_face_dict, known_face_encodings, known_face_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ğŸŒŸ ä¿å­˜äººè„¸ç¼–ç åˆ° JSON\n",
    "# ==========================\n",
    "def save_encodings(known_face_dict):\n",
    "    data_to_save = {}\n",
    "    for label, encoding_array in known_face_dict.items():\n",
    "        data_to_save[label] = encoding_array.tolist()  # è½¬æˆ JSON å¯ä¿å­˜çš„ list\n",
    "    \n",
    "    with open(ENCODINGS_FILE, 'w') as f:\n",
    "        json.dump(data_to_save, f)\n",
    "    \n",
    "    print(f\"ğŸ’¾ å·²ä¿å­˜ {len(data_to_save)} ä¸ªäººç‰©çš„ç¼–ç æ•°æ®åˆ° {ENCODINGS_FILE}\")\n",
    "    \n",
    "    \n",
    "\n",
    "# åˆå§‹åŒ–å‡½æ•°\n",
    "def init_blip_and_spacy():\n",
    "    global processor, model, nlp  # å¼•ç”¨å…¨å±€å˜é‡\n",
    "    try:\n",
    "        print(\"æ­£åœ¨åŠ è½½ BLIP æ¨¡å‹å’Œå¤„ç†å™¨...\")\n",
    "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        print(\"BLIP æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½å®Œæˆï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"åŠ è½½ BLIP æ¨¡å‹æ—¶å‡ºé”™: {e}\")\n",
    "        processor, model = None, None\n",
    "\n",
    "    try:\n",
    "        print(\"æ­£åœ¨æ£€æµ‹å’ŒåŠ è½½ SpaCy è‹±æ–‡æ¨¡å‹...\")\n",
    "        download(\"en_core_web_sm\")  # åªä¸‹è½½ä¸€æ¬¡\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"SpaCy è‹±æ–‡æ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"åŠ è½½ SpaCy æ¨¡å‹æ—¶å‡ºé”™: {e}\")\n",
    "        nlp = None\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ğŸŒŸ å¤„ç†å•å¼ å›¾ç‰‡\n",
    "# ==========================\n",
    "def process_new_photo(photo_path):\n",
    "    global next_label_id\n",
    "    try:\n",
    "        # 1. æ‰“å¼€å¹¶è½¬æ¢å›¾ç‰‡\n",
    "        with Image.open(photo_path) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "            image_array = np.array(img)\n",
    "        \n",
    "        # 2. äººè„¸è¯†åˆ«\n",
    "        encodings = face_recognition.face_encodings(image_array,model='large')\n",
    "\n",
    "        if not encodings:\n",
    "            print(f\"âš ï¸ å›¾ç‰‡ {photo_path} ä¸åŒ…å«äººè„¸ï¼Œè·³è¿‡å¤„ç†ã€‚\")\n",
    "            return\n",
    "        \n",
    "        print(f\"âœ… è¯†åˆ«åˆ° {len(encodings)} å¼ äººè„¸ï¼Œå¼€å§‹åˆ†ç±»...\")\n",
    "\n",
    "        for encoding in encodings:\n",
    "            if len(known_face_encodings) == 0:\n",
    "                # æ²¡æœ‰äººè„¸æ•°æ®ï¼Œç›´æ¥æ–°å»ºäººç‰©\n",
    "                person_label = next_label_id\n",
    "                next_label_id += 1\n",
    "\n",
    "                known_face_dict[person_label] = encoding\n",
    "                known_face_encodings.append(encoding)\n",
    "                known_face_labels.append(person_label)\n",
    "                print(f\"ğŸ†• æœªæ£€æµ‹åˆ°å·²æœ‰äººç‰©ï¼Œæ–°å»ºäººç‰© {person_label}\")\n",
    "\n",
    "            else:\n",
    "                # æ¯”è¾ƒä¸å·²çŸ¥äººè„¸çš„ç›¸ä¼¼åº¦\n",
    "                distances = face_recognition.face_distance(known_face_encodings, encoding)\n",
    "                min_distance = np.min(distances)\n",
    "                best_match_index = np.argmin(distances)\n",
    "\n",
    "                if min_distance < 0.4:  # é˜ˆå€¼å¯ä»¥è°ƒæ•´\n",
    "                    person_label = known_face_labels[best_match_index]\n",
    "                    # print(f\"ğŸ‘Œ åŒ¹é…åˆ°äººç‰© {person_label}ï¼Œè·ç¦»ä¸º {min_distance:.2f}\")\n",
    "                else:\n",
    "                    # æ–°äººç‰©\n",
    "                    person_label = next_label_id\n",
    "                    next_label_id += 1\n",
    "\n",
    "                    known_face_dict[person_label] = encoding\n",
    "                    known_face_encodings.append(encoding)\n",
    "                    known_face_labels.append(person_label)\n",
    "                    # print(f\"ğŸ†• æ–°å»ºäººç‰© {person_label}ï¼Œè·ç¦»ä¸º {min_distance:.2f}\")\n",
    "            \n",
    "            # ä¿å­˜ç…§ç‰‡åˆ°åˆ†ç±»ç›®å½•\n",
    "            person_folder = os.path.join(SORTED_DIR, f\"äººç‰©{person_label}\")\n",
    "            os.makedirs(person_folder, exist_ok=True)\n",
    "            shutil.copy(photo_path, person_folder)\n",
    "        \n",
    "        # å¤„ç†å®Œå›¾ç‰‡ï¼Œä¿å­˜æœ€æ–°çš„ encodings\n",
    "        save_encodings(known_face_dict)\n",
    "        return person_label\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¤„ç† {photo_path} æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "def get_decimal_from_dms(dms, ref):\n",
    "    \"\"\"Convert GPS coordinates in DMS to decimal format.\"\"\"\n",
    "    degrees = dms[0][0] / dms[0][1]\n",
    "    minutes = dms[1][0] / dms[1][1]\n",
    "    seconds = dms[2][0] / dms[2][1]\n",
    "\n",
    "    decimal = degrees + (minutes / 60.0) + (seconds / 3600.0)\n",
    "\n",
    "    if ref in ['S', 'W']:\n",
    "        decimal = -decimal\n",
    "    return decimal\n",
    "\n",
    "def reverse_geocode(lat, lon):\n",
    "    \"\"\"Use geopy to reverse geocode latitude and longitude to address.\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"photo_metadata_app\")\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True, language='en')\n",
    "        if location:\n",
    "            return location.address\n",
    "        else:\n",
    "            return \"Address not found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving address: {e}\"\n",
    "\n",
    "def extract_exif_data(image_path):\n",
    "    # æ‰“å¼€å›¾ç‰‡æ–‡ä»¶\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # è·å– EXIF ä¿¡æ¯\n",
    "    exif_data = img._getexif()\n",
    "\n",
    "    # æå–æ‹æ‘„æ—¶é—´\n",
    "    timestamp = None\n",
    "    if exif_data and 36867 in exif_data:\n",
    "        timestamp = exif_data[36867]  # DateTimeOriginal\n",
    "        timestamp = datetime.strptime(timestamp, '%Y:%m:%d %H:%M:%S')\n",
    "\n",
    "    # ä½¿ç”¨ piexif åŠ è½½æ›´è¯¦ç»†çš„ EXIF æ•°æ®\n",
    "    exif_dict = piexif.load(img.info['exif']) if 'exif' in img.info else None\n",
    "\n",
    "    # æå–ç›¸æœº/è®¾å¤‡ä¿¡æ¯\n",
    "    camera_model = None\n",
    "    if exif_dict:\n",
    "        model = exif_dict['0th'].get(piexif.ImageIFD.Model, None)\n",
    "        make = exif_dict['0th'].get(piexif.ImageIFD.Make, None)\n",
    "\n",
    "        camera_model = \"\"\n",
    "        if make:\n",
    "            camera_model += make.decode('utf-8') + \" \"\n",
    "        if model:\n",
    "            camera_model += model.decode('utf-8')\n",
    "\n",
    "    # æå– GPS ä¿¡æ¯\n",
    "    gps_info = exif_dict.get('GPS', None) if exif_dict else None\n",
    "    latitude = longitude = None\n",
    "    address = None\n",
    "    if gps_info:\n",
    "        gps_latitude = gps_info.get(piexif.GPSIFD.GPSLatitude)\n",
    "        gps_latitude_ref = gps_info.get(piexif.GPSIFD.GPSLatitudeRef).decode('utf-8')\n",
    "        gps_longitude = gps_info.get(piexif.GPSIFD.GPSLongitude)\n",
    "        gps_longitude_ref = gps_info.get(piexif.GPSIFD.GPSLongitudeRef).decode('utf-8')\n",
    "\n",
    "        if gps_latitude and gps_latitude_ref and gps_longitude and gps_longitude_ref:\n",
    "            latitude = get_decimal_from_dms(gps_latitude, gps_latitude_ref)\n",
    "            longitude = get_decimal_from_dms(gps_longitude, gps_longitude_ref)\n",
    "\n",
    "            # åå‘åœ°ç†ç¼–ç è·å–åœ°å€\n",
    "            address = reverse_geocode(latitude, longitude)\n",
    "\n",
    "    # è¿”å›æå–çš„ä¿¡æ¯\n",
    "    return {\n",
    "        'Timestamp': timestamp,\n",
    "        'Latitude': latitude,\n",
    "        'Longitude': longitude,\n",
    "        'Address': address,\n",
    "        'Camera/Device': camera_model\n",
    "    }\n",
    "\n",
    "# ç”Ÿæˆå›¾ç‰‡æè¿°\n",
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        raw_image = Image.open(image_path).convert('RGB')\n",
    "        inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "        out = model.generate(**inputs)\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        print(f\"Caption generation error for {image_path}: {e}\")\n",
    "        return \"No description available.\"\n",
    "    \n",
    "def extract_noun_tags(text):\n",
    "    \"\"\"\n",
    "    æå–è‹±æ–‡æ–‡æœ¬ä¸­çš„å…³é”®è¯ï¼ˆåè¯å’Œä¸“æœ‰åè¯ï¼‰\n",
    "\n",
    "    å‚æ•°:\n",
    "        text (str): è¾“å…¥çš„è‹±æ–‡æ–‡æœ¬\n",
    "\n",
    "    è¿”å›:\n",
    "        tags (list): å»é‡ä¸”å°å†™å¤„ç†åçš„å…³é”®è¯åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # åˆ†è¯ & è¯æ€§æ ‡æ³¨\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # åªä¿ç•™åè¯ï¼ˆNOUNï¼‰å’Œä¸“æœ‰åè¯ï¼ˆPROPNï¼‰\n",
    "    tags = [token.text for token in doc if token.pos_ in ['NOUN', 'PROPN']]\n",
    "\n",
    "    # å»é‡ & å°å†™ï¼ˆå¯æ ¹æ®éœ€æ±‚å–æ¶ˆå°å†™å¤„ç†ï¼‰\n",
    "    tags = list(set(tag.lower() for tag in tags))\n",
    "    \n",
    "    print(f\"Image event tags: {tags}\")\n",
    "\n",
    "    return tags\n",
    "\n",
    "\n",
    "# å¤„ç†å›¾ç‰‡\n",
    "def process_images(img_path):\n",
    "    # æå–å…ƒä¿¡æ¯\n",
    "    metadata = extract_exif_data(img_path)\n",
    "    \n",
    "    # ç”Ÿæˆæè¿°\n",
    "    caption = generate_caption(img_path)\n",
    "\n",
    "    #Aoto-tagging\n",
    "    tags = extract_noun_tags(caption)\n",
    "    \n",
    "    #äººè„¸è¯†åˆ« \n",
    "    person_label = process_new_photo(img_path)\n",
    "    \n",
    "    # âœ… å¹³é“º JSON æ ¼å¼\n",
    "    photo_info = {\n",
    "        'Timestamp': metadata.get('Timestamp'),\n",
    "        'Latitude': metadata.get('Latitude'),\n",
    "        'Longitude': metadata.get('Longitude'),\n",
    "        'Address': metadata.get('Address'),\n",
    "        'Camera/Device': metadata.get('Camera/Device'),\n",
    "        'Caption': caption,\n",
    "        'AutoTags': tags,\n",
    "        'PersonLabel': person_label\n",
    "    }\n",
    "    \n",
    "    print(f\"Processed {img_path}: {caption} @ {photo_info.get('Address')} on {photo_info.get('Timestamp')}\") \n",
    "    \n",
    "    return photo_info"
   ],
   "id": "7d4b2e575328b714",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T03:30:21.643180Z",
     "start_time": "2025-04-14T03:30:14.595222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# ğŸŒŸ åˆå§‹åŒ–å·²çŸ¥äººè„¸æ•°æ®\n",
    "# ==========================\n",
    "known_face_dict, known_face_encodings, known_face_labels = load_encodings()\n",
    "# è°ƒç”¨åˆå§‹åŒ–å‡½æ•°\n",
    "# è°ƒç”¨åˆå§‹åŒ–ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰\n",
    "init_blip_and_spacy()\n"
   ],
   "id": "4516cb627c34f649",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸåŠ è½½ 11 ä¸ªå·²çŸ¥äººç‰©æ•°æ®ã€‚\n",
      "æ­£åœ¨åŠ è½½ BLIP æ¨¡å‹å’Œå¤„ç†å™¨...\n",
      "BLIP æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½å®Œæˆï¼\n",
      "æ­£åœ¨æ£€æµ‹å’ŒåŠ è½½ SpaCy è‹±æ–‡æ¨¡å‹...\n",
      "\u001B[38;5;2mâœ” Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001B[38;5;3mâš  Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "SpaCy è‹±æ–‡æ¨¡å‹åŠ è½½å®Œæˆï¼\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T03:30:32.475287Z",
     "start_time": "2025-04-14T03:30:24.153772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "process_images(\"photos/15.jpg\")"
   ],
   "id": "bdba0f0b5a7914cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image event tags: ['man', 'fountain', 'front']\n",
      "âœ… è¯†åˆ«åˆ° 7 å¼ äººè„¸ï¼Œå¼€å§‹åˆ†ç±»...\n",
      "ğŸ’¾ å·²ä¿å­˜ 11 ä¸ªäººç‰©çš„ç¼–ç æ•°æ®åˆ° face_encodings.json\n",
      "Processed photos/15.jpg: a man standing in front of a fountain @ Window of the World, Shennan Boulevard, Huaxia Jie, Shahe Sub-district, Nanshan District, Shenzhen, Guangdong Province, 518000, China on 2025-01-31 18:03:58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Timestamp': datetime.datetime(2025, 1, 31, 18, 3, 58),\n",
       " 'Latitude': 22.536686111111113,\n",
       " 'Longitude': 113.97063611111112,\n",
       " 'Address': 'Window of the World, Shennan Boulevard, Huaxia Jie, Shahe Sub-district, Nanshan District, Shenzhen, Guangdong Province, 518000, China',\n",
       " 'Camera/Device': 'vivo iQOO Neo8 Pro',\n",
       " 'Caption': 'a man standing in front of a fountain',\n",
       " 'AutoTags': ['man', 'fountain', 'front'],\n",
       " 'PersonLabel': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import JSONResponse\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    global known_face_dict, known_face_encodings, known_face_labels\n",
    "    known_face_dict, known_face_encodings, known_face_labels = load_encodings()\n",
    "    init_blip_and_spacy()\n",
    "\n",
    "# ä¸Šä¼ å›¾ç‰‡æ¥å£\n",
    "@app.post(\"/process_image\")\n",
    "async def process_image(file: UploadFile = File(...)):\n",
    "    try:        \n",
    "        # è°ƒç”¨å¤„ç†å‡½æ•°\n",
    "        result = process_images(file)\n",
    "\n",
    "        # è¿”å› JSON\n",
    "        return JSONResponse(content=result)\n",
    "\n",
    "    except Exception as e:\n",
    "        return JSONResponse(status_code=500, content={\"error\": str(e)})"
   ],
   "id": "b92e14409d1831f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:47:37.939061Z",
     "start_time": "2025-04-07T06:47:26.342179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8123/process_image\"\n",
    "files = {'file': open('photos/13.jpg', 'rb')}\n",
    "response = requests.post(url, files=files)\n",
    "print(response.json())"
   ],
   "id": "be7e7d2cc4af0e19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Timestamp': '2025-02-28 21:14:13', 'Latitude': 22.585355, 'Longitude': 113.950085, 'Address': 'æ²™æ²³è¥¿è·¯è¾…è·¯, ä¹ç¥¥å²­, Xili Sub-District, Nanshan District, Guangdong Province, 518000, China', 'Camera/Device': 'Xiaomi 23013RK75C', 'Caption': 'a boy is cooking a meal in a pan', 'AutoTags': ['meal', 'pan', 'boy'], 'PersonLabel': None}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:03:15.887620Z",
     "start_time": "2025-05-21T13:03:12.035245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# video_generator.py\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import json\n",
    "import chardet\n",
    "from PIL import Image, ImageFilter\n",
    "from moviepy.editor import (\n",
    "    ImageSequenceClip, AudioFileClip, CompositeVideoClip,\n",
    "    concatenate_videoclips, VideoFileClip, vfx\n",
    ")\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def get_config():\n",
    "    config_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.json')\n",
    "    with open(config_file, 'rb') as f:\n",
    "        encoding_result = chardet.detect(raw_data := f.read())\n",
    "        encoding = encoding_result['encoding']\n",
    "    return json.loads(raw_data.decode(encoding))\n",
    "\n",
    "def transform_image(img, t, x_speed, y_speed, move_on_x, move_positive):\n",
    "    original_size = img.size\n",
    "    crop_width = img.width * 0.8\n",
    "    crop_height = img.height * 0.8\n",
    "    if move_on_x:\n",
    "        left = min(x_speed * t, img.width - crop_width) if move_positive else max(img.width - crop_width - x_speed * t, 0)\n",
    "        upper = (img.height - crop_height) / 2\n",
    "    else:\n",
    "        upper = min(y_speed * t, img.height - crop_height) if move_positive else max(img.height - crop_height - y_speed * t, 0)\n",
    "        left = (img.width - crop_width) / 2\n",
    "    cropped_img = img.crop((left, upper, left + crop_width, upper + crop_height))\n",
    "    return cropped_img.resize(original_size)\n",
    "\n",
    "def generate_video(image_paths: list[str]) -> str:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    config = get_config()\n",
    "\n",
    "    temp_dir = os.path.join(current_dir, 'temp')\n",
    "    video_dir = os.path.join(current_dir, 'video')\n",
    "    voice_path = os.path.join(current_dir, 'bgm', 'bgm.mp3')\n",
    "\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    fps = config['fps']\n",
    "    enlarge_background = config['enlarge_background']\n",
    "    duration = config['duration']\n",
    "\n",
    "    bgm_audio = AudioFileClip(voice_path)\n",
    "    clips = []\n",
    "\n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        im = Image.open(img_path)\n",
    "        effect_type = random.choice([0, 1])\n",
    "        if effect_type == 0:\n",
    "            x_speed = (im.width - im.width * 0.8) / duration\n",
    "            y_speed = 0\n",
    "            move_on_x = True\n",
    "            move_positive = random.choice([True, False])\n",
    "        else:\n",
    "            x_speed = 0\n",
    "            y_speed = (im.height - im.height * 0.8) / duration\n",
    "            move_on_x = False\n",
    "            move_positive = random.choice([True, False])\n",
    "\n",
    "        n_frames = int(fps * duration)\n",
    "        frames_foreground = [np.array(transform_image(im, t / fps, x_speed, y_speed, move_on_x, move_positive)) for t in range(n_frames)]\n",
    "        img_foreground = ImageSequenceClip(frames_foreground, fps=fps)\n",
    "\n",
    "        img_blur = im.filter(ImageFilter.GaussianBlur(radius=30))\n",
    "        if enlarge_background:\n",
    "            img_blur = img_blur.resize((int(im.width * 1.1), int(im.height * 1.1)), Image.Resampling.LANCZOS)\n",
    "        frames_background = [np.array(img_blur)] * n_frames\n",
    "        img_background = ImageSequenceClip(frames_background, fps=fps)\n",
    "\n",
    "        final_clip = CompositeVideoClip(\n",
    "            [img_background.set_position(\"center\"), img_foreground.set_position(\"center\")],\n",
    "            size=img_blur.size\n",
    "        )\n",
    "\n",
    "        final_clip = final_clip.set_duration(duration)\n",
    "        temp_clip_path = os.path.join(temp_dir, f'temp_{idx}.mp4')\n",
    "        final_clip.write_videofile(temp_clip_path, logger=None)\n",
    "        clips.append(VideoFileClip(temp_clip_path))\n",
    "        gc.collect()\n",
    "\n",
    "    final_video = concatenate_videoclips(clips, method=\"compose\")\n",
    "    final_video = final_video.set_audio(bgm_audio.subclip(0, final_video.duration))\n",
    "    output_path = os.path.join(video_dir, f'output_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.mp4')\n",
    "    final_video.write_videofile(output_path, logger=None)\n",
    "    return output_path\n"
   ],
   "id": "7d5b96f5291c30bf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T14:40:38.544973Z",
     "start_time": "2025-05-13T14:40:38.351610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import uvicorn\n",
    "# app.py\n",
    "import os\n",
    "import shutil\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import FileResponse\n",
    "from typing import List\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), \"image\")\n",
    "UPLOAD_FOLDER = os.path.join(os.getcwd(), \"image\")\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "@app.post(\"/generate_video/\")\n",
    "async def generate_video_api(files: List[UploadFile] = File(...)):\n",
    "    saved_files = []\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(UPLOAD_FOLDER, file.filename)\n",
    "        with open(file_path, \"wb\") as f:        \n",
    "            content = await file.read()\n",
    "            f.write(content)\n",
    "        saved_files.append(file_path)\n",
    "\n",
    "    try:\n",
    "        output_video_path = generate_video(saved_files)\n",
    "        return FileResponse(output_video_path, media_type=\"video/mp4\", filename=os.path.basename(output_video_path))\n",
    "    finally:\n",
    "        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "        for path in saved_files:\n",
    "            os.remove(path)\n",
    "            \n",
    "    "
   ],
   "id": "8edd1bd56cc1a8cc",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 35\u001B[0m\n\u001B[0;32m     31\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m saved_files:\n\u001B[0;32m     32\u001B[0m             os\u001B[38;5;241m.\u001B[39mremove(path)\n\u001B[1;32m---> 35\u001B[0m \u001B[43muvicorn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mapp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhost\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m0.0.0.0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8124\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\uvicorn\\main.py:579\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001B[0m\n\u001B[0;32m    577\u001B[0m         Multiprocess(config, target\u001B[38;5;241m=\u001B[39mserver\u001B[38;5;241m.\u001B[39mrun, sockets\u001B[38;5;241m=\u001B[39m[sock])\u001B[38;5;241m.\u001B[39mrun()\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 579\u001B[0m         \u001B[43mserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    580\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m    581\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# pragma: full coverage\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\uvicorn\\server.py:66\u001B[0m, in \u001B[0;36mServer.run\u001B[1;34m(self, sockets)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, sockets: \u001B[38;5;28mlist\u001B[39m[socket\u001B[38;5;241m.\u001B[39msocket] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39msetup_event_loop()\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserve\u001B[49m\u001B[43m(\u001B[49m\u001B[43msockets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msockets\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\runners.py:33\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(main, debug)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03mThis function runs the passed coroutine, taking care of\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;124;03m    asyncio.run(main())\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m events\u001B[38;5;241m.\u001B[39m_get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 33\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m     34\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masyncio.run() cannot be called from a running event loop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m coroutines\u001B[38;5;241m.\u001B[39miscoroutine(main):\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma coroutine was expected, got \u001B[39m\u001B[38;5;132;01m{!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(main))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T14:48:40.784738Z",
     "start_time": "2025-05-21T14:48:20.083562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "# æ¥å£åœ°å€ï¼ˆè¯·æ ¹æ®éœ€è¦æ›¿æ¢ host å’Œ portï¼‰\n",
    "url = \"http://localhost:8123/generate_video/\"\n",
    "\n",
    "# è¦ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„ï¼ˆæ›¿æ¢æˆä½ è‡ªå·±çš„ï¼‰\n",
    "image_paths = [\n",
    "    \"image/output_1.png\",\n",
    "    \"image/output_2.png\",\n",
    "    \"image/output_3.png\",\n",
    "    \"image/output_4.png\",\n",
    "    \"image/output_5.png\",\n",
    "    \"image/output_6.png\",\n",
    "    \"img/IMG_20230909_010443.png\", \n",
    "    \"img/IMG_20230909_030556.png\"\n",
    "]\n",
    "\n",
    "# image_paths = [\"img/IMG_20230909_010443.png\", \"img/IMG_20230909_030556.png\"]\n",
    "\n",
    "# æ„å»ºæ–‡ä»¶åˆ—è¡¨ï¼Œæ ¼å¼ä¸ºå…ƒç»„åˆ—è¡¨\n",
    "files = [('files', (os.path.basename(path), open(path, 'rb'), 'image/jpeg')) for path in image_paths]\n",
    "\n",
    "# å‘é€ POST è¯·æ±‚\n",
    "response = requests.post(url, files=files)\n",
    "\n",
    "# å¤„ç†å“åº”\n",
    "if response.status_code == 200:\n",
    "    output_path = \"result_video.mp4\"\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"âœ… è§†é¢‘ä¿å­˜æˆåŠŸï¼š{output_path}\")\n",
    "else:\n",
    "    print(f\"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}\")\n",
    "    print(response.text)\n",
    "    \n"
   ],
   "id": "21735846d4ab330c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è§†é¢‘ä¿å­˜æˆåŠŸï¼šresult_video.mp4\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T14:31:50.115228Z",
     "start_time": "2025-05-21T14:31:50.066853Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a4db1261bd298ed3",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Software Engineering\\\\team-project-25spring-9\\\\Process_Image\\\\files'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfiles\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:3431\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(fp, mode, formats)\u001B[0m\n\u001B[0;32m   3428\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(os\u001B[38;5;241m.\u001B[39mfspath(fp))\n\u001B[0;32m   3430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[1;32m-> 3431\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3432\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   3433\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'D:\\\\Software Engineering\\\\team-project-25spring-9\\\\Process_Image\\\\files'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T15:10:46.448609Z",
     "start_time": "2025-05-13T15:10:43.699597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "# url = \"http://localhost:8123/generate_capetion/\"\n",
    "# url = \"http://localhost:8123/auto_tag/\"\n",
    "# url = \"http://localhost:8123/extract_exif/\"\n",
    "# url = \"http://localhost:8123/face_recognition/\"\n",
    "url = \"http://localhost:8123/process_image/\"\n",
    "files = {'file': open('image/output_2.png', 'rb')}\n",
    "response = requests.post(url, files=files)\n",
    "print(response.json())"
   ],
   "id": "4d37e3fcd0768861",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person_label': 13}\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
